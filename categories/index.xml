<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rahul Pathak Blog</title>
    <link>http://rahulpathakgit.github.io/categories/index.xml</link>
    <description>Recent content on Rahul Pathak Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Your Name</copyright>
    <atom:link href="/categories/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deep Learning Through Recurrent Neural Networks</title>
      <link>http://rahulpathakgit.github.io/post/rnn/</link>
      <pubDate>Sun, 12 Mar 2017 00:19:04 +0530</pubDate>
      
      <guid>http://rahulpathakgit.github.io/post/rnn/</guid>
      <description>&lt;p&gt;If you think through this process you&amp;rsquo;ll start to find a few funny properties. For example what if we made a good action in frame 50 (bouncing the ball back correctly), but then missed the ball in frame 150? If every single action is now labeled as bad (because we lost), wouldn&amp;rsquo;t that discourage the correct bounce on frame 50? You&amp;rsquo;re right - it would. However, when you consider the process over thousands/millions of games, then doing the first bounce correctly makes you slightly more likely to win down the road, so on average you&amp;rsquo;ll see more positive than negative updates for the correct bounce and your policy will end up doing the right thing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update: December 9, 2016 - alternative view&lt;/strong&gt;. In my explanation above I use the terms such as &amp;ldquo;fill in the gradient and backprop&amp;rdquo;, which I realize is a special kind of thinking if you&amp;rsquo;re used to writing your own backprop code, or using Torch where the gradients are explicit and open for tinkering. However, if you&amp;rsquo;re used to Theano or TensorFlow you might be a little perplexed because the code is oranized around specifying a loss function and the backprop is fully automatic and hard to tinker with. In this case, the following alternative view might be more intuitive. In vanilla supervised learning the objective is to maximize \( \sum_i \log p(y_i \mid x_i) \) where \(x_i, y_i \) are training examples (such as images and their labels). Policy gradients is exactly the same as supervised learning with two minor differences: 1) We don&amp;rsquo;t have the correct labels \(y_i\) so as a &amp;ldquo;fake label&amp;rdquo; we substitute the action we happened to sample from the policy when it saw \(x_i\), and 2) We modulate the loss for each example multiplicatively based on the eventual outcome, since we want to increase the log probability for actions that worked and decrease it for those that didn&amp;rsquo;t. So in summary our loss now looks like \( \sum_i A_i \log p(y_i \mid x_i) \), where \(y_i\) is the action we happened to sample and \(A_i\) is a number that we call an &lt;strong&gt;advantage&lt;/strong&gt;. In the case of Pong, for example, \(A_i\) could be 1.0 if we eventually won in the episode that contained \(x_i\) and -1.0 if we lost. This will ensure that we maximize the log probability of actions that led to good outcome and minimize the log probability of those that didn&amp;rsquo;t. So reinforcement learning is exactly like supervised learning, but on a continuously changing dataset (the episodes), scaled by the advantage, and we only want to do one (or very few) updates based on each sampled dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;More general advantage functions&lt;/strong&gt;. I also promised a bit more discussion of the returns. So far we have judged the &lt;em&gt;goodness&lt;/em&gt; of every individual action based on whether or not we win the game. In a more general RL setting we would receive some reward \(r_t\) at every time step. One common choice is to use a discounted reward, so the &amp;ldquo;eventual reward&amp;rdquo; in the diagram above would become \( R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} \), where \(\gamma\) is a number between 0 and 1 called a discount factor (e.g. 0.99). The expression states that the strength with which we encourage a sampled action is the weighted sum of all rewards afterwards, but later rewards are exponentially less important. In practice it can can also be important to normalize these. For example, suppose we compute \(R_t\) for all of the 20,000 actions in the batch of 100 Pong game rollouts above. One good idea is to &amp;ldquo;standardize&amp;rdquo; these returns (e.g. subtract mean, divide by standard deviation) before we plug them into backprop. This way we&amp;rsquo;re always encouraging and discouraging roughly half of the performed actions. Mathematically you can also interpret these tricks as a way of controlling the variance of the policy gradient estimator. A more in-depth exploration can be found &lt;a href=&#34;http://arxiv.org/abs/1506.02438&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deriving Policy Gradients&lt;/strong&gt;. I&amp;rsquo;d like to also give a sketch of where Policy Gradients come from mathematically. Policy Gradients are a special case of a more general &lt;em&gt;score function gradient estimator&lt;/em&gt;. The general case is that when we have an expression of the form \(E_{x \sim p(x \mid \theta)} [f(x)] \) - i.e. the expectation of some scalar valued score function \(f(x)\) under some probability distribution \(p(x;\theta)\) parameterized by some \(\theta\). Hint hint, \(f(x)\) will become our reward function (or advantage function more generally) and \(p(x)\) will be our policy network, which is really a model for \(p(a \mid I)\), giving a distribution over actions for any image \(I\). Then we are interested in finding how we should shift the distribution (through its parameters \(\theta\)) to increase the scores of its samples, as judged by \(f\) (i.e. how do we change the network&amp;rsquo;s parameters so that action samples get higher rewards). We have that:&lt;/p&gt;

&lt;div&gt;
$$
\begin{align}
\nabla_{\theta} E_x[f(x)] &amp;= \nabla_{\theta} \sum_x p(x) f(x) &amp; \text{definition of expectation} \\
&amp; = \sum_x \nabla_{\theta} p(x) f(x) &amp; \text{swap sum and gradient} \\
&amp; = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) &amp; \text{both multiply and divide by } p(x) \\
&amp; = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) &amp; \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
&amp; = E_x[f(x) \nabla_{\theta} \log p(x) ] &amp; \text{definition of expectation}
\end{align}
$$
&lt;/div&gt;

&lt;p&gt;To put this in English, we have some distribution \(p(x;\theta)\) (I used shorthand \(p(x)\) to reduce clutter) that we can sample from (e.g. this could be a gaussian). For each sample we can also evaluate the score function \(f\) which takes the sample and gives us some scalar-valued score. This equation is telling us how we should shift the distribution (through its parameters \(\theta\)) if we wanted its samples to achieve higher scores, as judged by \(f\). In particular, it says that look: draw some samples \(x\), evaluate their scores \(f(x)\), and for each \(x\) also evaluate the second term \( \nabla_{\theta} \log p(x;\theta) \). What is this second term? It&amp;rsquo;s a vector - the gradient that&amp;rsquo;s giving us the direction in the parameter space that would lead to increase of the probability assigned to an \(x\). In other words if we were to nudge \(\theta\) in the direction of \( \nabla_{\theta} \log p(x;\theta) \) we would see the new probability assigned to some \(x\) slightly increase. If you look back at the formula, it&amp;rsquo;s telling us that we should take this direction and multiply onto it the scalar-valued score \(f(x)\). This will make it so that samples that have a higher score will &amp;ldquo;tug&amp;rdquo; on the probability density stronger than the samples that have lower score, so if we were to do an update based on several samples from \(p\) the probability density would shift around in the direction of higher scores, making highly-scoring samples more likely.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
